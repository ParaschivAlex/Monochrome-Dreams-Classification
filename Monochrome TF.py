# -*- coding: utf-8 -*-
"""Proiect IA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ILDWDmetqk_FuZc2F-xsZMcw4-msxlP6
"""

#!unzip ai-unibuc-24-22-2021.zip

import cv2
import numpy as np
import os
import sys
import tensorflow as tf
from sklearn.model_selection import train_test_split
import glob
import matplotlib.pyplot as plt
from keras.models import Sequential
from tensorflow.keras import datasets, layers, models

label_train = []
train_name_files = []
f = open("train.txt", "r") #am citit la fel din fisier aici doar ca am schimbat ordinea, mai intai am citit ce este in fisierele .txt si dupa imaginile
for line in f:
    separator = line.split(",")
    train_name_files.append(separator[0])
    label_train.append(int(separator[1]))
f.close()
print(label_train[:6])
label_train = np.array(label_train)

label_validation = []
validation_name_files = []
f = open("validation.txt", "r")
for line in f:
    separator = line.split(",")
    validation_name_files.append(separator[0])
    label_validation.append(int(separator[1]))
f.close()
label_validation = np.array(label_validation)

test_name_files = []
f = open("test.txt", "r")
for line in f:
    test_name_files.append(line[0:10])
f.close()

def load_train_data():
    train_images = [] #nu mai pot citi cu glob pentru ca rulez pe gpu pe google colab si glob lua imaginile in ordine aleatoare si era un haos total
    for i in range(len(label_train)): #asa ca citesc fiecare imagine cu un i de la 0 la numarul de imagini
        train_images.append(cv2.imread("train/" + train_name_files[i], cv2.IMREAD_GRAYSCALE)) #citesc cu cv2 care ia pathname-ul si al doilea argument un flag care semnaleaza ca avem poze monochromate(pe tonuri de gri)
    train_images = np.array(train_images) # fac vectorul de imagini numpy array
    return train_images


def load_validation_data():
    validation_images = []
    for i in range(len(label_validation)):
        validation_images.append(cv2.imread("validation/" + validation_name_files[i], cv2.IMREAD_GRAYSCALE))
    validation_images = np.array(validation_images)
    return validation_images


def load_test_data():
    test_images = []
    for i in range(len(test_name_files)):
        test_images.append(cv2.imread("test/" + test_name_files[i], cv2.IMREAD_GRAYSCALE))
    test_images = np.array(test_images)
    return test_images


loaded_train_images = load_train_data() #incarcarea datelor este la fel
print(len(loaded_train_images))
loaded_validation_images = load_validation_data()
loaded_test_images = load_test_data()
print(loaded_validation_images)

loaded_train_images = loaded_train_images / 255.0 #aici pentru ca nu mai citesc cu glob, impartirea valorilor pixelilor nu se mai face automat la 255 
loaded_validation_images = loaded_validation_images / 255.0 #dar fac eu impartirea la 255 manual
loaded_test_images = loaded_test_images / 255.0

print(loaded_train_images) #afisez valori sa vad ca e totul ok
print(loaded_train_images.shape)
# plt.imshow(loaded_train_images[-1])
# plt.show()

length_train, train_x_axis, train_y_axis = loaded_train_images.shape #ca la toate solutiile, iau shapeul datelor citite pentru a face reshape mai tarziu
length_validation, validation_x_axis, validation_y_axis = loaded_validation_images.shape
length_test, test_x_axis, test_y_axis = loaded_test_images.shape

load_reshaped_train = loaded_train_images.reshape(length_train,32,32,1) #acum reshapuiesc vectorul de date din 3-dimensional in 4-dimensional
load_reshaped_validation = loaded_validation_images.reshape(length_validation,32,32,1)
load_reshaped_test = loaded_test_images.reshape(length_test,32,32,1)

print(load_reshaped_train.shape)
print(label_train.shape)

""" #Aici am vrut sa fac pe schema din curs (cursul 7 1:25:40) in care avem: conv,relu,conv,relu,pool x3 FC dar nu dadea ceva wow (am schimbat si relu cu elu pentru a avea gradient mai bun pentru x<0)
cnn = models.Sequential([
    layers.Conv2D(filters=32, kernel_size=(5,5), activation='elu', input_shape=(32, 32,1)),
    layers.BatchNormalization(axis=-1),

    layers.Conv2D(filters=64, kernel_size=(5,5), activation='elu'),
    layers.BatchNormalization(axis=-1),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(filters=128, kernel_size=(5,5), activation='elu'),
    layers.BatchNormalization(axis=-1),
    
    layers.Conv2D(filters=256, kernel_size=(5,5), activation='elu'),
    layers.BatchNormalization(axis=-1),
    layers.MaxPooling2D((2, 2)),
    
    layers.Flatten(),
    layers.Dense(200, activation='elu'),
    layers.Dropout(0.25),
    layers.Dense(9, activation='softmax')
])
"""

#Pentru modelul acesta am stat toata ziua de duminica si am tot invartit valori pentru a obtine un rezultat cat mai bun si pentru a evita overfitting/underfitting
cnn = models.Sequential([ #Am ales Sequential pentru ca grupeaza o stiva liniara de straturi intr-un model si are functiile date in curs 
    layers.Conv2D(filters=32, kernel_size=(5,5), activation='elu', input_shape=(32, 32, 1)), #am luat 32 de filtre 5x5 pentru ca avem imagini 32x32 si pentru ca am vazut si in cursul 7 acest exemplu
    #folosesc elu pentru a avea gradient mai bun pentru x<0 desi acum se face o functie exponentiala
    layers.BatchNormalization(axis=-1), #la inceput nu faceam aceasta normalizare si obtineam valori de aproximativ 0.85. Am vazut in curs aceasta normalizare, am aplicat-o si am obtinut rezultate mult mai bune
    layers.MaxPooling2D((2, 2)),#fac pooling 2x2, scad dimensiunea spatiala (aplic tot o operatie de filtrare dar nu mai e de convolutie ca mai sus, pe ce am eu este maximul si este cea mai populara)
    #strideul il las default

    layers.Conv2D(filters=32, kernel_size=(5,5), activation='elu', input_shape=(32, 32,1)), #repet acelasi procedeu
    layers.BatchNormalization(axis=-1),
    layers.MaxPooling2D((2, 2)),
    
    
    layers.Flatten(), #aplatizez datele, neafectandu-le dimensiunea
    layers.Dense(200, activation='elu'), #pusesem dense cu 1024 de unitati si elu initial ca aveam 32x32 imaginea si facea overfitting si am tot scazut pana am gasit o valoare optima (zic eu)
    layers.Dropout(0.8), #dezactivez neuroni random (ii fac 0) cu o frecventa de 0.25, in curs aveam ca sfat practic ca o valoare ok pentru 10 clase este 0.25 asa ca am luat aceeasi valoare
    layers.Dense(9, activation='softmax') # iau 9 categorii si cu softmax transform vectorul de date intr-un vector de probabilitati (pentru clasificare)
])#Aveam initial dropout 0.25 si 50 de epoci si era overfit. Am modificat cu 0.8 si 35, nu este un overfit sau e foarte putin si pe kaggle ajung la 0.915 in loc de 0.908 dar asta este, era ultima submisie.

 #acum compilez modelul definit
cnn.compile(optimizer='adam', #optimizez cu algoritmul adam, am incercat cu sgd ca in curs initial si apoi am incercat adam
              loss='sparse_categorical_crossentropy',  #calculez pierderea intre etichete si predictii (sparse_c_c face un vector cu cele mai probabile categorii din cele 9)
              metrics=['accuracy']) #calculez cat de des am predictiile==etichete, ia 2 valori total si nr si impart total la nr. Este la fel ca la SVM cand aveam o functie de calculat acuratetea

cnn.fit(load_reshaped_train, label_train, epochs=35) #antrenez modelul pe 35 de epoci

cnn.evaluate(load_reshaped_validation,label_validation) #evaluez pe datele de validare
validare = cnn.predict_classes(load_reshaped_validation)
print(validare)

#cand am obtinut o solutie peste 0.90 am vrut sa antrenez modelul pentru datele de train + validare pentru a avea o solutie si mai buna dar nu mai aveam submisii
sample_sub = cnn.predict_classes(load_reshaped_test)

def prediction_matrix(label_true, label_predicted):  # aici afisez matricea de predictii
    num_classes = max(max(label_true), max(label_predicted)) + 1  # iau numarul de clase posibile, puteam sa ii dau 9 eu
    pred_matrix = np.zeros((num_classes, num_classes))  # face o matrice 9x9 initializata cu 0

    for i in range(len(label_true)):  # iau i = numarul de labeluri date (de imagini)
        pred_matrix[int(label_true[i]), int(label_predicted[i])] += 1  # daca prezicerea este corecta crestem valoarea pe diagonala principala, daca nu in afara ei ( [i][j] i -> ce trebuia prezis si j-> ce a prezis)
    return pred_matrix


print(prediction_matrix(label_validation, validare))

g = open("sample_submission.txt", "w")
print(len(sample_sub), len(test_name_files))
g.write("id,label\n")
for i in range (len(sample_sub)):
    g.write(str(test_name_files[i]) + "," + str(sample_sub[i]) + '\n')

g.close()
